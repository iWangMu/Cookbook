## 线性回归

机器学习领域大多数任务都与**预测（prediction）**有关，预测结果是一个数值时，是**回归（regression）**问题，预测数据属于某个类别，则是**分类**问题。

**回归（regression）**是为一个或多个自变量与因变量之间关系建模的一类方法。

**线性回归（linear regression）**假设：自变量 $\vec x$ 和因变量 $y$ 之间的关系是线性的，即 $y$ 可以表示为 $\vec x$ 中元素的加权和，允许存在一些噪声；假设任何噪声都比较正常，例如噪声遵循正态分布。

### 机器学习的一些术语

- 训练数据集（training data set），训练集（training set）
- 样本（sample），数据点（data point），数据样本（data instance）：数据集中的每行数据
- 标签（label），目标（target）：预测的目标
- 特征（feature），协变量（covariate）：预测所依据的自变量

$n$ 表示数据集中的样本数量，索引为 $i$ 的样本，其特征是 ${\vec x}^{(i)} = [{x_1}^{(i)}, {x_2}^{(i)}, ..., {x_d}^{(i)}]^T$ ，对应的标签是 $y^{(i)}$ 。

### 线性模型

输入包含 $d$ 个特征，预测结果 $\hat y$ （ $y$ 的估计值）可以表示：
$$
\hat y = w_1x_1 + w_2x_2 + ... + w_dx_d + b
$$
$\vec w=[w_1, w_2, ...w_d]$ 称为**权重（weight）**，$b$ 称为**偏置（bis）**、偏移量（offset）或截距（intercept）。

将特征放到向量 $\vec x \in \mathbb{R}^d$ 中，简化表达上述模型：
$$
\hat y = \vec w^T\vec x + b
$$
向量 $\vec x$ 对应单个数据样本的特征，数据集的 $n$ 个样本可以使用矩阵 $\boldsymbol{X} \in \mathbb{R}^{n\times{d}}$ 表示，$\boldsymbol{X}$ 的每一行表示一个数据样本，每一列是一种特征。对于特征集合 $\boldsymbol{X}$ ，预测值 $\hat {\vec y} \in \mathbb{R}$ 可以通过矩阵-向量乘法表示为：
$$
\hat{\vec y} = \boldsymbol{X}\vec w + b
$$
线性模型的**目标**是找到一组权重向量 $\vec w$ 和偏置 $b$ ，当给定从$\boldsymbol{X}$ 的同分布中取样的新样本特征时，这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。

- 损失函数：一种模型质量的度量方式。
- 梯度下降：一种能够更新模型以提高模型预测质量的方法。

### 损失函数

**损失函数（loss function）**能够量化目标的实际值与预测值之间的差距。

回归问题中最常用的损失函数是**平方误差函数**，样本 $i$ 的预测值为 $\hat y^{(i)}$，真实标签为 $y^{(i)}$，平方误差的定义如下：
$$
l^{(i)}(\vec w, b) = \dfrac{1}{2}({\hat y}^{(i)} - y^{(i)}) ^ 2
$$
为了度量模型在整个数据集上的质量，我们需要计算在训练集 $n$ 个样本上的损失均值：
$$
L(\vec w, b) = \dfrac{1}{n}\sum_{i=1}^{n}l^{(i)}(\vec w, b) = \dfrac{1}{n}\sum_{i=1}^n\dfrac{1}{2}(\vec w^T\vec x^{(i)} + b - y^{(i)}) ^ 2
$$
在训练模型时，我们希望寻找一组参数 $(\vec w^*, b^*)$​ ，这组参数能最小化在所有训练样本上的总损失，即：
$$
\vec w^*, b^* = \underset{\vec w, b}{argmin}\text{ } L(\vec w, b)
$$




































