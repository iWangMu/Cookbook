## 线性回归

机器学习领域大多数任务都与**预测（prediction）**有关，预测结果是一个数值时，是**回归（regression）**问题，预测数据属于某个类别，则是**分类**问题。

**回归（regression）**是为一个或多个自变量与因变量之间关系建模的一类方法。

**线性回归（linear regression）**假设：自变量 $\vec x$ 和因变量 $y$ 之间的关系是线性的，即 $y$ 可以表示为 $\vec x$ 中元素的加权和，允许存在一些噪声；假设任何噪声都比较正常，例如噪声遵循正态分布。

### 机器学习的一些术语

- 训练数据集（training data set），训练集（training set）
- 样本（sample），数据点（data point），数据样本（data instance）：数据集中的每行数据
- 标签（label），目标（target）：预测的目标
- 特征（feature），协变量（covariate）：预测所依据的自变量

$n$ 表示数据集中的样本数量，索引为 $i$ 的样本，其特征是 ${\vec x}^{(i)} = [{x_1}^{(i)}, {x_2}^{(i)}, ..., {x_d}^{(i)}]^T$ ，对应的标签是 $y^{(i)}$ 。

### 线性模型

输入包含 $d$ 个特征，预测结果 $\hat y$ （ $y$ 的估计值）可以表示：
$$
\hat y = w_1x_1 + w_2x_2 + ... + w_dx_d + b
$$
$\vec w=[w_1, w_2, ...w_d]$ 称为**权重（weight）**，$b$ 称为**偏置（bis）**、偏移量（offset）或截距（intercept）。

将特征放到向量 $\vec x \in \mathbb{R}^d$ 中，简化表达上述模型：
$$
\hat y = \vec w^T\vec x + b
$$
向量 $\vec x$ 对应单个数据样本的特征，数据集的 $n$ 个样本可以使用矩阵 $\boldsymbol{X} \in \mathbb{R}^{n\times{d}}$ 表示，$\boldsymbol{X}$ 的每一行表示一个数据样本，每一列是一种特征。对于特征集合 $\boldsymbol{X}$ ，预测值 $\hat {\vec y} \in \mathbb{R}$ 可以通过矩阵-向量乘法表示为：
$$
\hat{\vec y} = \boldsymbol{X}\vec w + b
$$
线性模型的**目标**是找到一组权重向量 $\vec w$ 和偏置 $b$ ，当给定从$\boldsymbol{X}$ 的同分布中取样的新样本特征时，这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。

- 损失函数：一种模型质量的度量方式。
- 梯度下降：一种能够更新模型以提高模型预测质量的方法。



回归（regression）



分类

预测（prediction）；预测一个值——回归问题；预测一个类别——分类问题







训练数据集（training data set），训练集（training set）

样本（sample），数据点（data point），数据样本（data instance）

标签（label）或目标（target）

特征（feature）或协变量（covariate）

`n`表示数据集中的样本数



线性回归（linear regression）
$$
price = w_{area} * area + w_{age} * age + b
$$

$$
Y_1
$$

啊啊$$
$$
\vec{a}
w_{area}
$$
行内公式：$w_{abc}$​



$\vec {\hat w}$

损失函数，模型质量的度量方式

梯度下降，能够更新模型以提高模型预测质量的方法
$$
l^{(i)}(\vec w, b) = \dfrac{1}{2}({\hat y}^{(i)} - y^{(i)}) ^ 2
$$
$\dfrac{1}{2}$是为了在对损失函数求导之后常数系数为$1$。
